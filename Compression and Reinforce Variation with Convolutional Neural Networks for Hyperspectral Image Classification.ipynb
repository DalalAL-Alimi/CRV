{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compression and Reinforce Variation with Convolutional Neural Networks for Hyperspectral Image Classification",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DalalAL-Alimi/CRV/blob/main/Compression and Reinforce Variation with Convolutional Neural Networks for Hyperspectral Image Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQvvDnmEEC-8"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2T6J6Kx01eX"
      },
      "source": [
        "pip install spectral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3am1a3PX0Bip"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "\n",
        "from operator import truediv\n",
        "\n",
        "from plotly.offline import init_notebook_mode\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sn\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRqBCt7A1u2-"
      },
      "source": [
        "## GLOBAL VARIABLES\n",
        "dataset = 'KSC'\n",
        "test_ratio = 0.8\n",
        "windowSize = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVpvbF2Y11tJ"
      },
      "source": [
        "def loadData(name):\n",
        "    data_path = os.path.join(os.getcwd(),'data')\n",
        "    \n",
        "    if name == 'IP':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "    elif name == 'SA':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
        "    elif name == 'PU':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
        "    elif name == 'KSC':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'KSC.mat'))['KSC']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'KSC_gt.mat'))['KSC_gt'] \n",
        "    \n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0jNaMkFZdZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extract_pixels(X, y):\n",
        "  q = X.reshape(-1, X.shape[2])\n",
        "  df = pd.DataFrame(data = q)\n",
        "  df = pd.concat([df, pd.DataFrame(data = y.ravel())], axis=1)\n",
        "  df.columns= [f'band{i}' for i in range(1, 1+X.shape[2])]+['class']\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plkjeR3E14y5"
      },
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PnDlEK31-lR"
      },
      "source": [
        "from skimage.morphology import reconstruction\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "def CRV(X, band_no, w, h, numComponents):\n",
        "\n",
        "  X_mean = X\n",
        "  X_mean['mean'] = X_mean.mean(axis=1)\n",
        "  Avr = [X_mean['mean']]\n",
        "  rep_Avr = np.repeat(Avr, band_no)\n",
        "  rep_Avr =np.reshape(rep_Avr, (-1,band_no))\n",
        "  main_X = X.iloc[:, :-1]\n",
        "  seed = main_X - rep_Avr\n",
        "  dilated = reconstruction(seed.values, main_X.values)\n",
        "  hdome = main_X - dilated\n",
        "\n",
        "\n",
        "  hdome_raw_mean = hdome\n",
        "  hdome_raw_mean = hdome_raw_mean.append(hdome_raw_mean.agg(['mean'], axis=0))\n",
        "  sort_hdome_raw_mean = hdome_raw_mean.sort_values(by='mean', axis=1, ascending=False)\n",
        "  Top_columns = sort_hdome_raw_mean[sort_hdome_raw_mean.columns[0:numComponents]]\n",
        "  columns_Names = Top_columns.columns\n",
        "\n",
        "  input_data = df[columns_Names]\n",
        "\n",
        "  scaler = QuantileTransformer(n_quantiles=15, random_state=0,output_distribution='normal') #\n",
        "  scaler = scaler.fit_transform(input_data)\n",
        "  scaler = np.reshape(scaler, (w, h, numComponents))\n",
        "\n",
        "  return scaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO7bawvV2Ajp"
      },
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3gP4Vx12CiC"
      },
      "source": [
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Y3bxQP2ERh"
      },
      "source": [
        "X1, y1 = loadData(dataset)\n",
        "band_no = X1.shape[2]\n",
        "h = X1.shape[1]\n",
        "w = X1.shape[0]\n",
        "K = 15  # The target number of bands\n",
        "X1.shape, y1.shape, h, w, band_no"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFv_5OTKFmu9"
      },
      "source": [
        "df = extract_pixels(X1, y1)\n",
        "X = df.iloc[:, :-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUcBxUiGG-Y"
      },
      "source": [
        "CRV_X = CRV(X, band_no, w, h, numComponents= K)\n",
        "X = CRV_X\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofkRRaJC2QGe"
      },
      "source": [
        "X, y = createImageCubes(X, y1, windowSize=windowSize)\n",
        "\n",
        "X.shape, y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUy_dX3S2SRe"
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X, y, test_ratio)\n",
        "\n",
        "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duRa5mTa2cdd"
      },
      "source": [
        "# Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz_m2dmc2XaD"
      },
      "source": [
        "Xtrain = Xtrain.reshape(-1, windowSize, windowSize, K, 1)\n",
        "\n",
        "Xtrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_h26MXj2hVN"
      },
      "source": [
        "ytrain = np_utils.to_categorical(ytrain)\n",
        "\n",
        "ytrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGFHHbca2kdI"
      },
      "source": [
        "S = windowSize\n",
        "L = K\n",
        "output_units = 9 if (dataset == 'PU' or dataset == 'PC') else 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXo3Zqx02nDB"
      },
      "source": [
        "## input layer\n",
        "input_layer = Input((S, S, L, 1))\n",
        "\n",
        "## convolutional layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer2)\n",
        "print(conv_layer3.shape)\n",
        "conv3d_shape = conv_layer3.shape\n",
        "conv_layer3 = Reshape((conv3d_shape[1], conv3d_shape[2], conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
        "conv_layer4 = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(conv_layer3)\n",
        "\n",
        "flatten_layer = Flatten()(conv_layer4)\n",
        "\n",
        "## fully connected layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = BatchNormalization()(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = BatchNormalization()(dense_layer2)\n",
        "output_layer = Dense(units=output_units, activation='softmax')(dense_layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AzXCSQm2piS"
      },
      "source": [
        "# define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJBaZ2uj28Bl"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udutJUd03ceg"
      },
      "source": [
        "# compiling the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1wzJjT63g5C"
      },
      "source": [
        "# checkpoint\n",
        "filepath = \"best-modelKSCQTBN15.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxXAgvdn3lWX"
      },
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "history = model.fit(x=Xtrain, y=ytrain, batch_size=100, epochs=100, callbacks=callbacks_list)\n",
        "training_end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViuUJMyVE-wE"
      },
      "source": [
        "print(\"Training time : \", training_end_time - training_start_time )\n",
        "model1_Training_time = training_end_time - training_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVF53Ck8Csl_"
      },
      "source": [
        "import os\n",
        "path = 'models'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfXDu4p5pSSp"
      },
      "source": [
        "# save model\n",
        "filename = 'model_1.h5'\n",
        "model.save(path +'/'+filename)\n",
        "print('>Saved %s' % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZvW8HN8HPGS"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUMIWU7UIdqP"
      },
      "source": [
        "ac = history.history['accuracy']\n",
        "accuracy_report = pd.DataFrame(ac)\n",
        "accuracy_report.to_csv('Model_accuracy.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdy0d5MGHW3d"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV6O62x_Hsiq"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training loss', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9qsT_hIy96"
      },
      "source": [
        "ls = history.history['loss']\n",
        "accuracy_report = pd.DataFrame(ls)\n",
        "accuracy_report.to_csv('Model_loss.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n44QWr6A30bB"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_zAgPRJ3pcx"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZy92ilz350w"
      },
      "source": [
        "Xtest = Xtest.reshape(-1, windowSize, windowSize, K, 1)\n",
        "Xtest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4cq0jKB38uG"
      },
      "source": [
        "ytest = np_utils.to_categorical(ytest)\n",
        "ytest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDRryyjT3_zf"
      },
      "source": [
        "Y_pred_test = model.predict(Xtest)\n",
        "y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
        "\n",
        "classification = classification_report(np.argmax(ytest, axis=1), y_pred_test)\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ5t7eT34DVU"
      },
      "source": [
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_CuJlf-4DYT"
      },
      "source": [
        "def reports (X_test,y_test,name):\n",
        "    start = time.time()\n",
        "    Y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    end = time.time()\n",
        "\n",
        "    TESTING_TIME = end - start\n",
        "    print(TESTING_TIME)\n",
        "    if name == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    elif name == 'SA':\n",
        "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
        "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
        "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
        "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
        "    elif name == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
        "                        'Self-Blocking Bricks','Shadows']\n",
        "    elif name == 'KSC':\n",
        "        target_names = [\"Scrub\", \"Willow swamp\",\n",
        "                        \"Cabbage palm hammock\", \"Cabbage palm/oak hammock\",\n",
        "                        \"Slash pine\", \"Oak/broadleaf hammock\",\n",
        "                        \"Hardwood swamp\", \"Graminoid marsh\", \"Spartina marsh\",\n",
        "                        \"Cattail marsh\", \"Salt marsh\", \"Mud flats\", \"Wate\"]\n",
        "    \n",
        "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "    ###################################\n",
        "    clsf_report = pd.DataFrame(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names, output_dict=True)).transpose()\n",
        "    clsf_report.to_csv(dataset+'_Model Classification Report.csv', index= True)\n",
        "    #########################\n",
        "    df_cm = pd.DataFrame(confusion, columns=np.unique(target_names), index = np.unique(target_names))\n",
        "    df_cm.index.name = 'Actual'\n",
        "    df_cm.columns.name = 'Predicted'\n",
        "    plt.figure(figsize = (10,8))\n",
        "    sn.set(font_scale=1.4)#for label size\n",
        "    sn.heatmap(df_cm, cmap=\"Reds\", annot=True,annot_kws={\"size\": 16}, fmt='d')\n",
        "    plt.savefig('confusion_matrix_model.png', dpi=300)\n",
        "    ##############\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    \n",
        "    return TESTING_TIME, classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1IZ_PN34L2i"
      },
      "source": [
        "TESTING_TIME, classification, confusion, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(Xtest,ytest,dataset)\n",
        "df_clas = classification\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "element_mean = np.mean(each_acc, axis=0)\n",
        "element_std = np.std(each_acc, axis=0)\n",
        "file_name = \"classification_Results.txt\"\n",
        "\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write('{} Training Time (%)'.format(model1_Training_time))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test Time (%)'.format(TESTING_TIME))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test loss (%)'.format(Test_loss))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}+{} mean_KAPPA ± std_KAPPA is:'.format(str(np.mean(kappa)), str(np.std(kappa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}+{} mean_OA ± std_OA is: (%)'.format(str(np.mean(oa)), str(np.std(oa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}+{} mean_AA ± std_AA is:'.format(str(np.mean(aa)), str(np.std(aa))))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} AMean of all elements in confusion matrix: (%)'.format(str(element_mean)))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Standard deviation of all elements in confusion matrix: (%)'.format(str(element_std)))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('==============================\\n')\n",
        "    x_file.write('{}classification Report:'.format(classification))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('===============================\\n')\n",
        "    x_file.write('{} confusion Matrix:'.format(confusion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO8pZgvH4Ptj"
      },
      "source": [
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    \n",
        "    return patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jtM42uQ4Pza"
      },
      "source": [
        "height = y1.shape[0]\n",
        "width = y1.shape[1]\n",
        "PATCH_SIZE = windowSize\n",
        "numComponents = K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8anZu_e94WDk"
      },
      "source": [
        "X = padWithZeros(CRV_X, PATCH_SIZE//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr_u0JRo4WHz"
      },
      "source": [
        "# calculate the predicted image\n",
        "outputs1 = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(y[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(X,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')                                   \n",
        "            prediction = (model.predict(X_test_image))\n",
        "            prediction = np.argmax(prediction, axis=1)\n",
        "            outputs1[i][j] = prediction+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD73NBUw4WKK"
      },
      "source": [
        "ground_truth = spectral.imshow(classes = y,figsize =(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH_2oLOE4n_O"
      },
      "source": [
        "predict_image1 = spectral.imshow(classes = outputs1.astype(int),figsize =(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5SrUj1w4rRT"
      },
      "source": [
        "spectral.save_rgb(\"predictions_KSC_model.png\", outputs1.astype(int), colors=spectral.spy_colors) \n",
        "spectral.save_rgb(\"ground_truth_KSC_model.png\", y, colors=spectral.spy_colors) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNW61EezLM7N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
